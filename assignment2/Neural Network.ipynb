{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for layer1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for layer2.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer2.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for layer1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for layer2.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer2.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302198, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302705, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301631, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302176, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.301959, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301919, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f4364dceba81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# You should expect loss to go down every epoch, even if it's slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Edu/dlcourse_ai/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             train_accuracy = self.compute_accuracy(self.dataset.train_X,\n\u001b[0;32m--> 119\u001b[0;31m                                                    self.dataset.train_y)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             val_accuracy = self.compute_accuracy(self.dataset.val_X,\n",
      "\u001b[0;32m~/Edu/dlcourse_ai/assignment2/trainer.py\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Edu/dlcourse_ai/assignment2/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Hint: some of the code of the compute_loss_and_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# can be reused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Edu/dlcourse_ai/assignment2/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3fe3f7db033e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 44.685838, Train accuracy: 0.300556, val accuracy: 0.312000\n",
      "Loss: 36.561844, Train accuracy: 0.508111, val accuracy: 0.507000\n",
      "Loss: 33.117321, Train accuracy: 0.533000, val accuracy: 0.519000\n",
      "Loss: 42.362450, Train accuracy: 0.553444, val accuracy: 0.543000\n",
      "Loss: 29.327403, Train accuracy: 0.564000, val accuracy: 0.560000\n",
      "Loss: 51.014493, Train accuracy: 0.434444, val accuracy: 0.446000\n",
      "Loss: 31.372024, Train accuracy: 0.590333, val accuracy: 0.579000\n",
      "Loss: 36.087072, Train accuracy: 0.554889, val accuracy: 0.536000\n",
      "Loss: 41.158377, Train accuracy: 0.579889, val accuracy: 0.597000\n",
      "Loss: 37.462522, Train accuracy: 0.528222, val accuracy: 0.534000\n",
      "Loss: 27.053407, Train accuracy: 0.559667, val accuracy: 0.529000\n",
      "Loss: 33.584782, Train accuracy: 0.611222, val accuracy: 0.629000\n",
      "Loss: 39.742323, Train accuracy: 0.547889, val accuracy: 0.554000\n",
      "Loss: 34.119332, Train accuracy: 0.571333, val accuracy: 0.578000\n",
      "Loss: 29.329932, Train accuracy: 0.620889, val accuracy: 0.612000\n",
      "Loss: 30.932090, Train accuracy: 0.610889, val accuracy: 0.601000\n",
      "Loss: 33.936622, Train accuracy: 0.647778, val accuracy: 0.638000\n",
      "Loss: 26.219535, Train accuracy: 0.624778, val accuracy: 0.604000\n",
      "Loss: 31.219553, Train accuracy: 0.586444, val accuracy: 0.595000\n",
      "Loss: 34.876066, Train accuracy: 0.632111, val accuracy: 0.615000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 43.025909, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.747209, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.902497, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 38.533364, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.887901, Train accuracy: 0.205000, val accuracy: 0.212000\n",
      "Loss: 41.695714, Train accuracy: 0.248889, val accuracy: 0.252000\n",
      "Loss: 39.984884, Train accuracy: 0.269778, val accuracy: 0.269000\n",
      "Loss: 37.989162, Train accuracy: 0.295222, val accuracy: 0.297000\n",
      "Loss: 36.217654, Train accuracy: 0.356000, val accuracy: 0.365000\n",
      "Loss: 38.091380, Train accuracy: 0.404889, val accuracy: 0.381000\n",
      "Loss: 36.485403, Train accuracy: 0.435333, val accuracy: 0.427000\n",
      "Loss: 37.664789, Train accuracy: 0.461333, val accuracy: 0.461000\n",
      "Loss: 35.149638, Train accuracy: 0.483889, val accuracy: 0.485000\n",
      "Loss: 34.093564, Train accuracy: 0.533444, val accuracy: 0.535000\n",
      "Loss: 31.074557, Train accuracy: 0.550222, val accuracy: 0.545000\n",
      "Loss: 31.652784, Train accuracy: 0.568333, val accuracy: 0.559000\n",
      "Loss: 36.920458, Train accuracy: 0.587333, val accuracy: 0.574000\n",
      "Loss: 26.401292, Train accuracy: 0.601222, val accuracy: 0.586000\n",
      "Loss: 30.897894, Train accuracy: 0.617111, val accuracy: 0.610000\n",
      "Loss: 33.141314, Train accuracy: 0.633444, val accuracy: 0.625000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331257, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.312689, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.352013, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.281259, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.318255, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271724, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287555, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.266984, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.174836, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.241221, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.236188, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.348734, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.915233, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.583214, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.544315, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.974078, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.887526, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.224990, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.648182, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.939845, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.356707, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.053296, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.422393, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.787918, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.547633, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.366432, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.786944, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.408971, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.318987, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.054497, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.842926, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.101934, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.459241, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.890398, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.447344, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.955192, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.734101, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.343399, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.790149, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.857078, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.574797, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.688604, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.348627, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.331810, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.607720, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.020149, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.457120, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.400689, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.212969, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.590764, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.761033, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.166806, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.323154, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.112378, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.464653, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.762400, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.615805, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.694215, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.804018, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.673810, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.342533, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.536271, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.639283, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.322796, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.755356, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.344114, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.969816, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.427253, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.621631, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.900606, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.612206, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.243027, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.328891, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.464560, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.299672, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.300595, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.917019, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.855252, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.680762, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.802403, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.392268, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.411512, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.264940, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.388486, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.272867, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.227964, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.158690, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.597522, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.312625, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.240897, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.975653, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.042632, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.637704, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.078369, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.080018, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.485108, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.256669, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.091171, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.280746, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.256626, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.412362, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.975545, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.000205, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.294573, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.185023, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.398662, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.291273, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.207423, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496267, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.367180, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.282977, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.289350, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277990, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.151881, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.065254, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.211179, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.442795, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.114440, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.392656, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.212609, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.407274, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277275, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.447365, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.518868, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.197768, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.377143, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.394275, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.433880, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.434295, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.361976, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.417682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.453007, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.422521, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.433070, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.446240, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.285291, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.433377, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.327648, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.676907, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280183, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.252145, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.292379, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.293571, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.359522, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.158496, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.465834, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.307570, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.366827, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.354085, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.155638, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.305777, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.282819, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255530, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.327063, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.832070, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.449632, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.483399, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.286773, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.949917, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.366293, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.889183, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.531747, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.596367, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.151637, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.190085, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.118814, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 0.127468, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 2.223458, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.181122, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 8.833978, Train accuracy: 0.800000, val accuracy: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 200, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.298858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.136486, Train accuracy: 0.241333, val accuracy: 0.245000\n",
      "Loss: 1.985082, Train accuracy: 0.325778, val accuracy: 0.310000\n",
      "Loss: 1.591526, Train accuracy: 0.428111, val accuracy: 0.415000\n",
      "Loss: 1.615959, Train accuracy: 0.528000, val accuracy: 0.537000\n",
      "Loss: 1.217635, Train accuracy: 0.568222, val accuracy: 0.562000\n",
      "Loss: 1.755578, Train accuracy: 0.627333, val accuracy: 0.620000\n",
      "Loss: 1.117275, Train accuracy: 0.644333, val accuracy: 0.642000\n",
      "Loss: 1.321737, Train accuracy: 0.670556, val accuracy: 0.676000\n",
      "Loss: 0.878124, Train accuracy: 0.701889, val accuracy: 0.678000\n",
      "Loss: 1.591998, Train accuracy: 0.706111, val accuracy: 0.678000\n",
      "Loss: 0.789112, Train accuracy: 0.721444, val accuracy: 0.700000\n",
      "Loss: 1.217160, Train accuracy: 0.738333, val accuracy: 0.712000\n",
      "Loss: 1.215820, Train accuracy: 0.713667, val accuracy: 0.690000\n",
      "Loss: 1.199619, Train accuracy: 0.747778, val accuracy: 0.693000\n",
      "Loss: 1.007476, Train accuracy: 0.753889, val accuracy: 0.714000\n",
      "Loss: 0.814586, Train accuracy: 0.763222, val accuracy: 0.725000\n",
      "Loss: 1.167187, Train accuracy: 0.782111, val accuracy: 0.731000\n",
      "Loss: 0.927530, Train accuracy: 0.793000, val accuracy: 0.731000\n",
      "Loss: 0.751642, Train accuracy: 0.784556, val accuracy: 0.733000\n",
      "Loss: 0.789757, Train accuracy: 0.799333, val accuracy: 0.740000\n",
      "Loss: 0.891992, Train accuracy: 0.806778, val accuracy: 0.737000\n",
      "Loss: 0.976222, Train accuracy: 0.802111, val accuracy: 0.730000\n",
      "Loss: 0.721132, Train accuracy: 0.822333, val accuracy: 0.745000\n",
      "Loss: 0.857258, Train accuracy: 0.821556, val accuracy: 0.744000\n",
      "Loss: 0.900689, Train accuracy: 0.810889, val accuracy: 0.729000\n",
      "Loss: 0.776300, Train accuracy: 0.831667, val accuracy: 0.749000\n",
      "Loss: 0.736700, Train accuracy: 0.831444, val accuracy: 0.736000\n",
      "Loss: 0.936246, Train accuracy: 0.833889, val accuracy: 0.728000\n",
      "Loss: 0.686387, Train accuracy: 0.844556, val accuracy: 0.759000\n",
      "Loss: 1.021334, Train accuracy: 0.845222, val accuracy: 0.750000\n",
      "Loss: 0.983171, Train accuracy: 0.845889, val accuracy: 0.746000\n",
      "Loss: 0.909016, Train accuracy: 0.865333, val accuracy: 0.757000\n",
      "Loss: 0.908005, Train accuracy: 0.859889, val accuracy: 0.747000\n",
      "Loss: 0.895481, Train accuracy: 0.865556, val accuracy: 0.767000\n",
      "Loss: 0.800279, Train accuracy: 0.861889, val accuracy: 0.742000\n",
      "Loss: 0.947806, Train accuracy: 0.872667, val accuracy: 0.769000\n",
      "Loss: 0.777612, Train accuracy: 0.871556, val accuracy: 0.763000\n",
      "Loss: 0.728362, Train accuracy: 0.867333, val accuracy: 0.760000\n",
      "Loss: 0.644548, Train accuracy: 0.867444, val accuracy: 0.756000\n",
      "Loss: 0.720523, Train accuracy: 0.869667, val accuracy: 0.763000\n",
      "Loss: 1.047450, Train accuracy: 0.868667, val accuracy: 0.765000\n",
      "Loss: 0.591820, Train accuracy: 0.889000, val accuracy: 0.762000\n",
      "Loss: 0.778309, Train accuracy: 0.881889, val accuracy: 0.767000\n",
      "Loss: 0.942667, Train accuracy: 0.892000, val accuracy: 0.765000\n",
      "Loss: 0.976287, Train accuracy: 0.887111, val accuracy: 0.774000\n",
      "Loss: 0.837076, Train accuracy: 0.873667, val accuracy: 0.752000\n",
      "Loss: 0.827937, Train accuracy: 0.893778, val accuracy: 0.763000\n",
      "Loss: 0.931906, Train accuracy: 0.869222, val accuracy: 0.751000\n"
     ]
    }
   ],
   "source": [
    "model_hyper_params = {'reg': 1e-3, 'hidden_layer_size': 128}\n",
    "trainer_hyper_params = {'num_epochs': 50, 'batch_size': 50, 'learning_rate': 1e-2, 'learning_rate_decay': 0.999}\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, **model_hyper_params)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), **trainer_hyper_params)\n",
    "    \n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "best_classifier = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11f3ec890>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.725000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
